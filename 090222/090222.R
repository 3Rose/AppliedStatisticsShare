library(MASS)
library(car)
library(rgl)
library(glmnet)
library(sp)           ## Data management
library(lattice)      ## Data management
library(gstat)
remove(list = setdiff(ls(), lsf.str()))
load("~/Documents/WeBeep Sync/APPLIED STATISTICS/Labs/Lab 5/mcshapiro.test.RData")
setwd("~/data_hdd2/MEGA/AppStat Exams (in English)/Exams (in English)/090222")

# Problem n.1
# The file nutrients.txt reports the nutrients (energy, proteins, fats, carbohydrates, sugars and fibers) of 295
# different breakfast cereals. Each statistical unit corresponds to a different brand and kind of cereals.

nutrients <- read.table("nutrients.txt", sep = " ", header = T)
head(nutrients)



# a) Decide whether it is appropriate to use the original variables or the standardized ones, and perform a Principal
# Component Analysis of the dataset. Report the loadings of the first principal component.

# Since there are no different units of measure of the kind km vs km^2 for example there's no
# real need to do it. For the sake of argument we proceed with the standardized variables.
nutrients <- scale(nutrients)


pc <- princomp(nutrients, scores = T)
summary(pc)

# Loadings of the first principal component
pc$loadings[,1]

# b) Report a plot of the loadings of the first 3 principal components and interpret them.

X11()
par(mfrow=c(1,3))

barplot(pc$loadings[,1])
barplot(pc$loadings[,2])
barplot(pc$loadings[,3])
dev.off()

# All the principal components seem to be some kind of contrast
# PC1 -> Contrast between (fiber, protein) and (kcal, carbs, sugar)
# PC2 -> Contrast between (carbs) and (fats, kcal)
# PC3 -> contrast between (fiber, sugar,carbs) and (energy, proteins)


# c) Report the biplot of the data along the first two principal components and describe what characterizes the
# cereals with positive scores for both the first and second components.
X11()
biplot(pc)
# We see that a greater score for PC1 comes with high levels of
# sugar, carbs and kcal
# Meanwhile high scores for PC2 (y axis) come from mostly fats and in part kcal
dev.off()
# d) Report the screeplot. Propose (with motivations) a dimensionality reduction of the dataset. Report the variance
# explained along the principal components retained.
plot(cumsum(pc$sd^2)/sum(pc$sd^2), type='b', axes=F, xlab='number of components', 
     ylab='contribution to the total variance', ylim=c(0,1))
abline(h=1, col='blue')
abline(h=0.8, lty=2, col='blue')
box()
axis(2,at=0:10/10,labels=0:10/10)
axis(1,at=1:ncol(nutrients),labels=1:ncol(nutrients),las=2)

# We could stick the first 2 or 3 compoents depending on how much of the original
# variability we want to retain. I keep the first 3 principal components (around 90% of the variability)

# e) Project a new cereal with 400kcal, 9g of proteins, 5g of fats, 100g of carbohydrates, 30g of sugar and 4g of fiber,
# on the reduced space identified at point (d).

# Projection onto the space of the first 3 PCs

entry <- data.frame(
  Energy_kcal = 400,
  Protein_g = 9,
  Fat_g = 5,
  Carb_g = 100,
  Sugar_g = 30,
  Fiber_g = 4
)

prediction <- predict(pc, entry)
predictionx
entry
pc$loadings[,1]

age <- nutrients
# Projection on the space generated by the first k principal components
x11(width=18, height=7)
par(mfrow=c(1,4))
#matplot(t(age), type='l', main = 'Data', ylim=range(age))
meanA <- colMeans(entry)
matplot(entry, type='l', main = '0 PC', lwd=2, ylim=range(entry))
scores.age <- prediction
projection <- entry
for(i in 1:3)
{
  projection <- projection + scores.age[,i] %*% t(load.age[,i])
  matplot(t(projection), type='l', main = paste('First', i, 'PCs'), ylim=range(entry))
  matplot(meanA, type='l', lwd=2, add=T)
}


# Problem n.2
# The file streaming.txt contains the data for 225 users of a music streaming service. The variable minutes contains
# the average minutes per day in 2021 for each user, while the variables artists the average number of single artists
# listened per day.

stream <- read.table("streaming.txt", sep = " ", header = T)
stream
# a) Perform a cluster analysis of the users by using a hierarchical clustering method (Euclidean distance and single
#                                                                                       linkage). Report the plot of the dendrogram and the number of clusters you deem appropriate to represent the
# data.

dataset <- stream
plot(dataset) 
# Distance matrix with euclidean distance
dataset.d <- dist(dataset, method='euclidean')

# Applied clustering (single, average, complete) to distances
cluster.sl <- hclust(dataset.d, method='single')

# We see the dendogram and choose 3 clusters, they seem to be reasonably
# large

plot(cluster.sl, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(cluster.sl, k=3)
cluster.sl <- cutree(cluster.sl, k=3) 

# We plot the 3 clusters
plot(dataset, col=ifelse(cluster.sl==1,'red',ifelse(cluster.sl==2,'blue','green')), pch=19)
# These clusters represent very well what we saw by eye, it performed well.  
  
# b) Report the centroids of the clusters, their size and the cophenetic coefficient.

# We compute the means of each cluster
# Number of elements in cluster 1
sum(cluster.sl == 1)
# Number of elements in cluster 2
sum(cluster.sl == 2)
# Number of elements in cluster 2
sum(cluster.sl == 3)

# For computing cluster center
C <- as.data.frame(matrix(nrow=0,ncol=3))
for(l in 1:length(unique(cluster.sl)))
  C <- rbind(C, colMeans(dataset[which(cluster.sl == l),]))
# We draw the centroids onto the plot
points(C, pch = 4, cex = 2, lwd = 2)

# And the cophenetic coefficient? i.e. how compatible is our clustering
# with the distance matrix?

cluster.sl <- hclust(dataset.d, method='single')
coph.sl <- cophenetic(cluster.sl)
coph.sl

# They're coherent
x11()
par(mfrow=c(1,2))
image(as.matrix(dataset.d), main='Euclidean', asp=1 )
image(as.matrix(coph.sl), main='Single', asp=1 )
dev.off()

es <- cor(dataset.d, coph.sl)
# The coefficient
c("Eucl-Single"=es)

# c) Provide Bonferroni intervals (global level 95%) for the mean of minutes and artists, within each of the clusters
# identified at point (a). Introduce and verify the appropriate assumptions.

cluster.sl <- cutree(cluster.sl, k=3) 
# We compute 6 Bonferroni intervals
i1 <- dataset[which(cluster.sl == 1),]
i2 <- dataset[which(cluster.sl == 2),]
i3 <- dataset[which(cluster.sl == 3),]

# Are our data gaussian?
P <- c(mcshapiro.test(i1)$p,
       mcshapiro.test(i2)$p,
       mcshapiro.test(i3)$p)
P
# Our data are gaussian by a large margin

i1 <- which(cluster.sl == 1)
i2 <- which(cluster.sl == 2)
i3 <- which(cluster.sl == 3)
k <- 6
alpha <- 0.05
# Our Bonferroni intervals
for(i in 1:3) {
  # Sample mean, covariance 
  x.mean   <- sapply(dataset[get(paste('i',i, sep='')),1:2],mean)
  x.cov    <- cov(dataset[get(paste('i',i, sep='')),1:2])
  x.invcov <- solve(x.cov)
  n <- sum(cluster.sl == i)
  cfr.t <- qt(1-alpha/(2*k),n-1)
  print(paste("Mean of cluster",i))
  ICMean <- cbind(inf = x.mean - cfr.t*sqrt(diag(x.cov)/n),
                  center = x.mean, 
                  sup = x.mean + cfr.t*sqrt(diag(x.cov)/n))
  print(ICMean)
}


# Problem n.3
# The file wine.txt reports the data on the alcohol content in 179 bottles of wine. For the alcohol content consider
# a linear model, accounting for the sugar content of grapes, and for type of wine (‘Red’, ‘Rose’, ‘White’):
#   alcoholg = β0,g + β1,g · sugar + ε,
# with ε ∼ 2 gN (0, σ ) and the grouping structure induced by the type of wine.

wine <- read.table("wine.txt", sep = " ", header =T, stringsAsFactors = TRUE)
head(wine)
# a) Estimate the parameters of the model ({β0,g , β1,g , σ}). Verify the model assumptions, reporting any plot you
# consider important.

levels(wine$type)
red <- rep(0,nrow(wine))
rose <- rep(0, nrow(wine))

red[which(wine$type == "Red")] <- 1
rose[which(wine$type == "Rose")] <- 1

fit <- lm(alcohol ~ red + rose + sugar:red + sugar:rose + sugar, data = wine)
summary(fit)

# We run diagnostics
par(mfrow=c(2,2))
plot(fit)
# No strange pattens and QQ plot show normality
shapiro.test(residuals(fit))
shapiro.test(rstudent(fit))


# b) Perform two statistical tests – each at level 1% – to verify if
# - there is a significant dependence of the mean alcohol content on the type of wine;
# - there is a significant dependence of the mean alcohol content on the sugar content.

# we test the type of wine

linearHypothesis(fit, rbind(c(0,1,0,0,0,0),
                            c(0,0,1,0,0,0),
                            c(0,0,0,0,1,0),
                            c(0,0,0,0,0,1)), c(0,0,0,0))

# Overall the type of wine does have an effect (we reject H0 at any level)

# And the sugar content?
linearHypothesis(fit, rbind(c(0,0,0,1,0,0),
                            c(0,0,0,0,1,0),
                            c(0,0,0,0,0,1)), c(0,0,0))
# Again it does have an effect, what about the singular interactions?

summary(fit)
fit2 <- lm(alcohol ~ rose + sugar + red:sugar + rose:sugar, data = wine)
summary(fit2)

# Again we try with more reductions
fit3 <- lm(alcohol ~ sugar + red:sugar + rose:sugar, data = wine)
summary(fit3)
# We keep this model

# c) Based on tests (b) or any other test deemed relevant, reduce the model and report the updated model
# parameters.
fit3 <- lm(alcohol ~ sugar + red:sugar + rose:sugar, data = wine)
summary(fit3)

# The coefficients
fit3$coefficients

# sigma^2 = 0.671

# d) Build a prediction interval at 99% for a new bottle of red wine made with grapes with 20 g of sugar.

Z0.new <- data.frame(sugar = 20, red = 1, rose = 0)

# Pred. int. for a new obs
Pred <- predict(fit3, Z0.new, interval='prediction', level=1-0.01)  
# Our prediction interval
Pred


# Problem n.4
# The file walesharks.txt collects the number y of sightings of wale sharks during January 2022 at 64 observatory
# points in the Indian Ocean. The dataset also reports the UTM coordinates si of those locations, and the Chlorophyll
# concentration x(si) (mg/m3 ) measured at the same locations. Consider for the variable y(si ), i = 1, ...64, the
# following model
# log[y(si)] = a0 + a1 · log[x(si )] + δ(si),
# with δ(si) a stationary residual.


sharks <- read.table("walesharks.txt", sep = " ", header = T)
sharks
# a) Estimate via generalized least squares the parameters a0 , a1 of the model. Report the model estimated for
# δ(si), and discuss the model assumptions (use an exponential model without nugget for δ(si )).

# We assume isotropy

coordinates(sharks) <- c('x','y')
sharks$sights <- log(sharks$sights)
sharks

# Build empirical variogram
v=variogram(sights ~ log.chlorofill, data=sharks)
plot(v,pch=19)

# Fit the variogram with exponential model
v.fit <- fit.variogram(v, vgm(0.4, "Exp", 100000))
plot(v, v.fit, pch = 3)

# We predict the zetas via GLS and then approximate the coefficients
# of the linear model

g.t <- gstat(formula = sights ~ log.chlorofill,
                     data = sharks, nmax = 50, model=v.fit, set = list(gls=1))
predict(g.t, sharks[1:2,], BLUE = TRUE)

zetas <- predict(g.t, sharks[4:5,], BLUE = TRUE)$var1.pred
regressors<-sharks$log.chlorofill[4:5]

# We approximate the coefficients by solving the linear system
# a0+ a1*regressors[1]=zetas[1]
# a0+ a1*regressors[2]=zetas[2]

A<-rbind(c(1,regressors[1]),c(1,regressors[2]))
coefs<-solve(A)%*%zetas
a0<-coefs[1]
a1<-coefs[2]
c(a0 = as.numeric(a0), a1 = as.numeric(a1))

# b) Provide a kriging prediction log[y ∗ (s0 )] of the log-number of sightings at an observatory point located close
# to the island of Fenfushi (South Ari Atoll, Maldives) s0 = (253844.8, 385997.7). For this purpose, use a
# point prediction of the log-transformed Chlorophyll concentration log[x(s0 )] obtained through a spatially
# stationary model (use a spherical model without nugget for log[x(s)]; report the estimated model, and the
#                   point prediction log[x(si)]∗).

# We first predict the log-chlorophyll concentration

s0.new <- data.frame(
  x = 253844.8,
  y = 385997.7
)
# Use a stationary model for log.chlorofill
coordinates(s0.new) <- c('x','y')
v=variogram(log.chlorofill ~ 1, data=sharks)
plot(v,pch=19)
# As the text suggest we use a spherical model
v.fit <- fit.variogram(v, vgm(3, "Sph", 100000))
plot(v, v.fit, pch = 3)
# Now we provide a point prediction of the log-chrolofill 

# The stationary model for log.chlorofill
# zeta.chlorofill = b_0 + b_1 * delta(s_i)
g.t <- gstat(formula = log.chlorofill ~ 1, data = sharks, model = v.fit)
# ordinary kriging (stationary model)
predict(g.t, s0.new)
# Our point prediction of log.chlorofill
s0.new$log.chlorofill <- 3.983747

# And we predict the log sights at those coordinates
# Use same model from before
v=variogram(sights ~ log.chlorofill, data=sharks)
plot(v,pch=19)

# Fit the variogram with exponential model (same as before)
v.fit <- fit.variogram(v, vgm(0.4, "Exp", 100000))
plot(v, v.fit, pch = 3)

# Our point prediction
g.t <- gstat(formula = sights ~ log.chlorofill, data = sharks, model = v.fit)
pred <- predict(g.t,s0.new, BLUE = FALSE)
pred
pred$var1.pred
# c) Report the kriging variance σ 2(s0 ) of the point prediction at point (b). Would you deem the variance σ 2 (s0)
# to be fully representative of the uncertainty associated with the prediction y ∗(s0 )?

# The kriging variance from point b)

pred$var1.var
# 0.4

# Considering that the variance of points where I do have data is
# practically zero
pred <- predict(g.t, sharks, BLUE = FALSE)
pred

# 0.4 is considerably bigger than the other variances in the dataset (approaching
# numerical zero). So it is consistent with the uncertainty associated to the
# new point.

