#successful tour.
library(MVN)
succesful <- geisha[which(clusters2==1),]
mvn(succesful)
unsuccesful <- geisha[which(clusters2 !=1),]
mvn(unsuccesful)
m.s <- colMeans(succesful)
m.u <- colMeans(unsuccesful)
n.s <- dim(succefsul)[1]
n.u <- dim(unsuccesful)[1]
cov(succesful)
n.s <- dim(succesful)[1]
n.u <- dim(unsuccesful)[1]
cov(succesful)
cov(unsuccesful)
cov(geisha)
Spooled <- (cov(succefsul)*(dim(succesful)[1]-1) +
cov(unsuccesful)*dim(unsuccesful)[1]-1)/
(dim(succefsul)[1] + dim(unsuccesful)[1] -2)
Spooled <- (cov(succesful)*(dim(succesful)[1]-1) +
cov(unsuccesful)*dim(unsuccesful)[1]-1)/
(dim(succefsul)[1] + dim(unsuccesful)[1] -2)
Spooled <- (cov(succesful)*(dim(succesful)[1]-1) +
cov(unsuccesful)*dim(unsuccesful)[1]-1)/
(dim(succesful)[1] + dim(unsuccesful)[1] -2)
cov(succesful) + cov(unsuccesful)
cov(geisha)
cov(succesful)
cov(unsuccesful)
succesful
S <- cov(geisha)
#bonferroni
ci1 <- c(
(m.s[1] -m.u[1]) - qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
)
#bonferroni
ci1 <- c(
(m.s[1] -m.u[1]) - qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
(m.s[1] -m.u[1]) + qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
)
#bonferroni
ci1 <- c(
(m.s[1] -m.u[1]) - qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
(m.s[1] -m.u[1]) + qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
)
qt(1-0.1/8,n.s+n.u-2)
sqrt(S[1,1]*(1/n.s + 1/n.u)
)
(m.s[1] -m.u[1])
#bonferroni
ci1 <- c(
(m.s[1] -m.u[1]) - qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u)),
(m.s[1] -m.u[1]) + qt(1-0.1/8,n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
)
ci2 <- c(
(m.s[2] -m.u[2]) - qt(1-0.1/8,n.s+n.u-2) * sqrt(S[2,2]*(1/n.s + 1/n.u)),
(m.s[2] -m.u[2]) + qt(1-0.1/8,n.s+n.u-2) * sqrt(S[2,2]*(1/n.s + 1/n.u))
)
ci1
ci2
m.s[2]
m.u[2]
#bonferroni
ci1 <- c(
(m.s[1] -m.u[1]) - qt(1-0.1/(2*2),n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u)),
(m.s[1] -m.u[1]) + qt(1-0.1/(2*2),n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
)
ci2 <- c(
(m.s[2] -m.u[2]) - qt(1-0.1/4,n.s+n.u-2) * sqrt(S[2,2]*(1/n.s + 1/n.u)),
(m.s[2] -m.u[2]) + qt(1-0.1/4,n.s+n.u-2) * sqrt(S[2,2]*(1/n.s + 1/n.u))
)
ci2
ci1
cov(succesful)
ci3 <- c(
(m.s[1]) - qt(1-0.1/4,n.s-1) * sqrt(cov(succesful)[1,1]*1/n.s),
(m.s[1]) + qt(1-0.1/4,n.s-1) * sqrt(cov(succesful)[1,1]*1/n.s)
)
ci4 <- c(
(m.s[2]) - qt(1-0.1/4,n.s-1) * sqrt(cov(succesful)[2,2]*1/n.s),
(m.s[2]) + qt(1-0.1/4,n.s-1) * sqrt(cov(succesful)[2,2]*1/n.s)
)
ci3
ci4
X11()
plot(geisha)
geisha.d <- dist(geisha, method='euclidean')
x11()
image(1:160,1:160,as.matrix(geisha.d), main='metrics: Euclidean', asp=1, xlab='duration', ylab='time')
cluster.sl <- hclust(geisha.d, method='single')
x11()
plot(cluster.sl, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
x11()
plot(cluster.sl, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(cluster.sl, k=2)
# Let's choose 2 clusters
# Fix k=2 clusters:
cluster.sl <- cutree(cluster.sl, k=2)
x11()
plot(geisha, col=ifelse(cluster.sl==1,'red','blue'), pch=19)
cluster.sl <- hclust(geisha.d, method='single')
coph.sl <- cophenetic(cluster.sl)
x11()
par(mfrow=c(1,2))
image(as.matrix(geisha.d), main='Euclidean', asp=1 )
image(as.matrix(coph.sl), main='Single', asp=1 )
es <- cor(geisha.d, coph.sl)
c("Eucl-Single"=es)
# Computation of cluster centers
C <- as.data.frame(matrix(nrow=0,ncol=2))
cluster.sl <- hclust(geisha.d, method='single')
cluster.sl <- cutree(cluster.sl, k=2)
for(l in 1:length(unique(cluster.sl)))
C <- rbind(C, colMeans(geisha[which(cluster.sl == l),]))
x11()
plot(geisha, col=ifelse(cluster.sl==1,'red','blue'), pch=19)
points(C, pch = 4, cex = 2, lwd = 2)
plot(geisha, col = clusters+9)
points(mean.c1[1], mean.c1[2], col='red')
points(mean.c2[1], mean.c2[2], pch = '*', lwd =20, col='blue')
x11()
plot(geisha, col=ifelse(cluster.sl==1,'red','blue'), pch=19)
points(C, pch = 4, cex = 2, lwd = 2)
plot(geisha, col = clusters+9)
points(mean.c1[1], mean.c1[2], col='red')
points(mean.c2[1], mean.c2[2], pch = '*', cex =3, col='blue')
points(mean.c1[1], mean.c1[2], cex=2, col='red')
plot(geisha, col = clusters+9)
points(mean.c1[1], mean.c1[2], cex=2, pch = '*', col='red')
points(mean.c2[1], mean.c2[2], pch = '*', cex =2, col='blue')
# Number of elements in cluster 1
sum(cluster.sl == 1)
# Number of elements in cluster 2
sum(cluster.sl == 2)
cluster.av <- hclust(geisha.d, method='average')
# The two clusters are more balanced
x11()
plot(cluster.av, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
# Choose 2 clusters
cluster.av <- cutree(cluster.av, k=2)
x11()
plot(geisha, col=ifelse(cluster.av==1,'red','blue'), pch=19)
# Number of elements in cluster 1
sum(cluster.av == 1)
# Number of elements in cluster 2
sum(cluster.av == 2)
# Test if populations are gaussian
P <- c(mcshapiro.test(geisha[which(cluster.av == 1),])$p,
mcshapiro.test(geisha[which(cluster.av == 2),])$p)
load("/home/rose/Desktop/exams/Exams (in English) - WINDOWS/Labs/Lab 5/mcshapiro.test.RData")
# Test if populations are gaussian
P <- c(mcshapiro.test(geisha[which(cluster.av == 1),])$p,
mcshapiro.test(geisha[which(cluster.av == 2),])$p)
P
# Assuming equal variance
# Both are gaussian (at level 5%)
# Simple Bonferroni intervals for the difference
k <- 4
n <- dim(geisha)[1]
alpha <- 0.1
cfr.t <- qt(1-alpha/(2*k),n-1)
geisha_successful <- geisha[which(cluster.av == 1),]
x.mean   <- colMeans(geisha_successful)
x.cov    <- cov(geisha_successful)
x.invcov <- solve(x.cov)
Bfsuccessful <- cbind(inf = x.mean - cfr.t*sqrt(diag(x.cov)/n),
sup = x.mean + cfr.t*sqrt(diag(x.cov)/n))
Bfsuccessful
# And for the mean of the difference
geisha_unsuccessful <- geisha[which(cluster.av == 2),]
n1 <- 75
n2 <- 85
p <- 2
t1 <- geisha_successful
t2 <- geisha_unsuccessful
t1.mean <- sapply(t1,mean)
t2.mean <- sapply(t2,mean)
t1.cov  <-  cov(t1)
t2.cov  <-  cov(t2)
Sp      <- ((n1-1)*t1.cov + (n2-1)*t2.cov)/(n1+n2-2)
cfr.t <- qt(1-alpha/(2*k),n1+n2-2)
Bf1 <- cbind(inf = (t1.mean[1]-t2.mean[1]) - cfr.t*sqrt(Sp[1,1]*(1/n1+1/n2)),
sup = (t1.mean[1]-t2.mean[1]) + cfr.t*sqrt(Sp[1,1]*(1/n1+1/n2)))
Bf2 <- cbind(inf = (t1.mean[2]-t2.mean[2]) - cfr.t*sqrt(Sp[2,2]*(1/n1+1/n2)),
sup = (t1.mean[2]-t2.mean[2]) + cfr.t*sqrt(Sp[2,2]*(1/n1+1/n2)))
Bfdiff <- rbind(Bf1, Bf2)
dimnames(Bfdiff)[[2]] <- c('inf','sup')
Bfdiff
Bfsuccessful
ci1
#bonferroni
ci1 <- c(
inf =(m.s[1] -m.u[1]) - qt(1-0.1/(2*2),n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u)),
sup =(m.s[1] -m.u[1]) + qt(1-0.1/(2*2),n.s+n.u-2) * sqrt(S[1,1]*(1/n.s + 1/n.u))
)
ci1
#bonferroni
ci1 <- c(
inf =(m.s[1] -m.u[1]) - qt(1-0.1/(2*2),n.s+n.u-2) * sqrt(Spooled[1,1]*(1/n.s + 1/n.u)),
sup =(m.s[1] -m.u[1]) + qt(1-0.1/(2*2),n.s+n.u-2) * sqrt(Spooled[1,1]*(1/n.s + 1/n.u))
)
ci2 <- c(
(m.s[2] -m.u[2]) - qt(1-0.1/4,n.s+n.u-2) * sqrt(Spooled[2,2]*(1/n.s + 1/n.u)),
(m.s[2] -m.u[2]) + qt(1-0.1/4,n.s+n.u-2) * sqrt(Spooled[2,2]*(1/n.s + 1/n.u))
)
ci1
ci2
Bfdiff
Bfsuccessful
ci1
ci2
ci3
ci4
garden <- read.table('garden.txt')
garden <- read.table('garden.txt')
head(garden)
attach(garden)
lm1 <- lm(extension ~ charps + maple + cherry + stones)
lm1 <- lm(extension ~ carps + maple + cherry + stones)
summary(lm1)
#assumptions
plot(lm1)
par(mfrow = c(2,2))
plot(lm1)
shapiro.test(lm1$residuals)
#parameters
lm1$coefficients
sigma <- sum(lm1$residuals^2)/lm1$df.residual
lm1$residuals
lm1$df.residual
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
library(car)
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
#at a level of confidence of 90% maples are not influencial
#at a level of confidence of 95% maples are influencial
linearHypothesis(lm1, c(0,0,0,1,0), 0)
linearHypothesis(lm1, rbind(c(0,1,0,0,0), c(0,0,0,0,1)), c(0,0))
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
linearHypothesis(lm1, c(0,0,0,1,0), 0)
linearHypothesis(lm1, rbind(c(0,1,0,0,0), c(0,0,0,0,1)), c(0,0))
#c
lm2 <- lm(extension ~ maple + cherry)
summary(lm2)
#osservo un p-value alto sul beta relativo ai cherry ->linearhypothesis
linearHypothesis(lm2, c(0,0,1), 0)
linearHypothesis(lm1, c(0,0,0,1,0), 0)
linearHypothesis(lm1, rbind(c(0,1,0,0,0), c(0,0,0,0,1)), c(0,0))
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
linearHypothesis(lm1, c(0,0,0,1,0), 0)
linearHypothesis(lm1, rbind(c(0,1,0,0,0), c(0,0,0,0,1)), c(0,0))
#c
lm2 <- lm(extension ~ carps + maple + stones)
summary(lm2)
linearHypothesis(lm1, rbind(c(0,1,0,0,0), c(0,0,0,0,1)), c(0,0))
#osservo un p-value alto sul beta relativo a carps ->linearhypothesis
linearHypothesis(lm2, c(0,1,0,0), 0)
# le carpe da sole non risultano influenti
lm3 <- lm(extension ~ maple + stones)
summary(lm3)
# abbiamo ora solo features particolarmente influenti, l'intercetta potrebbe essere 0
linearHypothesis(lm3, c(1,0,0),0)
sigma3 <- sum(lm3$residuals^2)/lm3$df.residual
betas3
#d
betas3 <- lm3$coefficients
sigma3 <- sum(lm3$residuals^2)/lm3$df.residual
betas3
sigma3
par(mfrow=c(2,2))
plot(lm3)
shapiro.test(lm3$residuals)
# Problem n.4
# The file garden.txt collects the number of carps, maple trees, cherry trees and stones, and the extension [m2 ] of
# 156 of garden in the Kantō region of Japan. Experts believe that, to achieve an overall balance of elements, the
# Japanese gardens follow the model
garden <- read.table("garden.txt", sep = " ", header = T)
View(garden)
# with E the extension of the garden, x1 , x2, x3 , x4 the number of carps, maple trees, cherry trees and stones
# respectively, and ε ∼ N (0, σ2 ).
colnames(garden)
# a) Estimate the 6 parameters of the model and verify the model assumptions. Evaluate the residuals of the model.
fit <- lm(extension ~ carps + maple + cherry + stones, data = garden)
summary(fit)
# The coefficients of the model
fit$coefficients
# The residuals appear to be gaussian
shapiro.test(fit$residuals)
#We run diagnostics
par(mfrow=c(2,2))
plot(fit)
linearHypothesis(fit,rbind(c(0,0,1,0,0),
c(0,0,0,1,0)),c(0,0))
# - there is statistical evidence of a dependence of the mean garden extension on lake elements (stones, carps).
# We run a linear hypothesis test
linearHypothesis(fit,rbind(c(0,1,0,0,0),
c(0,0,0,0,1)),c(0,0))
# d) Update the estimates of the parameters using the analysis at point (c).
linearHypothesis(fit,rbind(c(0,0,0,1,0)),0)
linearHypothesis(fit,rbind(c(0,0,1,0,0)),0)
pairs(garden)
fit2 <- lm(extension ~ stones + maple, data = garden)
X11()
par(mfrow = c(2,2))
plot(fit2)
X11()
par(mfrow = c(2,2))
plot(fit2)
garden <- read.table('garden.txt')
head(garden)
attach(garden)
lm1 <- lm(extension ~ carps + maple + cherry + stones)
summary(lm1)
#assumptions
par(mfrow = c(2,2))
plot(lm1)
shapiro.test(lm1$residuals)
help("shapiro.test")
par(mfrow = c(2,2))
plot(lm1)
#parameters
betas <- lm1$coefficients
sigma <- sum(lm1$residuals^2)/lm1$df.residual
summary(lm1)
sigma
sqrt(sigma)
library(car)
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
lm1 <- lm(extension ~ carps + maple + cherry + stones)
summary(lm1)
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
linearHypothesis(lm1, c(0,0,0,1,0), 0)
linearHypothesis(lm1, rbind(c(0,1,0,0,0), c(0,0,0,0,1)), c(0,0))
linearHypothesis(lm1, c(0,0,0,1,0), 0)
#b
linearHypothesis(lm1, c(0,0,1,0,0), 0)
#c
lm2 <- lm(extension ~ carps + maple + stones)
summary(lm2)
#osservo un p-value alto sul beta relativo a carps ->linearhypothesis
linearHypothesis(lm2, c(0,1,0,0), 0)
# le carpe da sole non risultano influenti
lm3 <- lm(extension ~ maple + stones)
summary(lm3)
lm3.2 <- lm((extension-mean(extension))~ I(maple- mean(maple)) + I(stones-mean(stones)))
summary(lm3.2)
par(mfrow=c(2,2))
plot(lm3.2)
par(mfrow=c(2,2))
plot(lm3)
lm3.3 <- lm(((extension-mean(extension))/cov(extension))~ I((maple- mean(maple))/cov(maple)) + I((stones-mean(stones))/cov(stones)))
cov(extension)
lm3.3 <- lm(((extension-mean(extension))/sd(extension))~ I((maple- mean(maple))/sd(maple)) + I((stones-mean(stones))/sd(stones)))
summary(lm3.3)
par(mfrow=c(2,2))
par(mfrow=c(2,2))
plot(lm3.3)
#a) Use a hierarchical clustering method based on Euclidean distance and single linkage to identify two groups of
#data (i.e., successful and unsuccessful tours). Report the centers of the clusters, the size of the clusters, the
#cophenetic coefficient and a qualitative plot of the results.
#b) Evaluate the quality of the clustering at point (a) and, in case you deem it unsatisfactory, repeat the procedure
#with another linkage at your choice.
#c) Identify the successful tours with the smaller group found with the clustering method at point (b). Having
#introduced and verified the needed assumptions, provide 4 Bonferroni intervals (global level 90%) for the dif-
#  ference in the mean characteristics of successful and unsuccessful tours, and for the mean characteristics of a
#successful tour.
#d) Comment the results at point (c) and suggest a successful strategy for Geisha hunting.
setwd("/home/rose/Desktop/exams/Exams (in English) - WINDOWS/030717/")
geisha <- read.table('geisha.txt')
head(geisha)
plot(geisha)
D <- dist(geisha, method = 'euclidian')
dendogram <- hclust(D, method = 'single')
dendogram
plot(dendogram)
dev.off
dev.off()
plot(dendogram)
clusters <- cutree(dendogram, 2)
clusters
mean.c1 <- colMeans(geisha[which(clusters == 1),])
mean.c2 <- colMeans(geisha[which(clusters != 1),])
plot(geisha, col = clusters+9)
points(mean.c1[1], mean.c1[2], cex=2, pch = '*', col='red')
points(mean.c2[1], mean.c2[2], pch = '*', cex =2, col='blue')
plot(dendogram)
rect.hclust(dendogram, 2)
clusters <- cutree(dendogram, 2)
clusters
mean.c1 <- colMeans(geisha[which(clusters == 1),])
mean.c2 <- colMeans(geisha[which(clusters != 1),])
plot(geisha, col = clusters+9)
points(mean.c1[1], mean.c1[2], cex=2, pch = '*', col='red')
points(mean.c2[1], mean.c2[2], pch = '*', cex =2, col='blue')
cophenetic(dendogram)
#b
dendogram2 <- hclust(D, method = 'complete')
dendogram3 <- hclust(D, method = "average")
dendogram4 <- hclust(D, method = 'ward.D2')
x11()
par(mfrow = c(1,3))
plot(dendogram2)
rect.hclust(dendogram2,2)
plot(dendogram3)
rect.hclust(dendogram3,2)
plot(dendogram4)
rect.hclust(dendogram4,2)
clusters2 <- cutree(dendogram2, 2)
clusters3 <- cutree(dendogram3, 2)
clusters4 <- cutree(dendogram4, 2)
x11()
par(mfrow = c(1,3))
plot(geisha, col=clusters2, main='complete')
points(colMeans(geisha[which(clusters2 == 1),])[1],
colMeans(geisha[which(clusters2 == 1),])[2], col='red', pch ='+', cex =4)
points(colMeans(geisha[which(clusters2 != 1),])[1],
colMeans(geisha[which(clusters2 != 1),])[2], col='gold', pch='-', cex=4)
plot(geisha, col = clusters3 +2, main= 'average')
plot(geisha, col = clusters4 +4, main='ward')
library(MVN)
succesful <- geisha[which(clusters2==1),]
mvn(succesful)
unsuccesful <- geisha[which(clusters2 !=1),]
hist(succesful)
succesful
mvn(unsuccesful)
m.s <- colMeans(succesful)
m.s <- colMeans(succesful)
m.u <- colMeans(unsuccesful)
n.s <- dim(succesful)[1]
n.u <- dim(unsuccesful)[1]
cov(succesful)
cov(unsuccesful)
S <- cov(geisha)
Spooled <- (cov(succesful)*(dim(succesful)[1]-1) +
cov(unsuccesful)*dim(unsuccesful)[1]-1)/
(dim(succesful)[1] + dim(unsuccesful)[1] -2)
Spooled
S
kimono <- read.table("kimono.txt")
head(kimono)
help("aov")
attach(kimono)
kimono.am <- aov(value ~ as.factor(city) + as.factor(type) + as.factor(city):as.factor(type))
summary(kimono.am)
g <- length(levels(as.factor(city)))
b <- length(levels(as.factor(type)))
n <- length(value)/(g*b)
N <- length(value)
P <- c(
shapiro.test(value[which(city == levels(as.factor(city))[1] & type == levels(as.factor(type))[1])])$p,
shapiro.test(value[which(city == levels(as.factor(city))[1] & type == levels(as.factor(type))[2])])$p,
shapiro.test(value[which(city == levels(as.factor(city))[2] & type == levels(as.factor(type))[1])])$p,
shapiro.test(value[which(city == levels(as.factor(city))[2] & type == levels(as.factor(type))[2])])$p
)
P
s11 <- var(value[which(city == levels(as.factor(city))[1] & type == levels(as.factor(type))[1])])
s12 <- var(value[which(city == levels(as.factor(city))[1] & type == levels(as.factor(type))[2])])
s21 <- var(value[which(city == levels(as.factor(city))[2] & type == levels(as.factor(type))[1])])
s22 <- var(value[which(city == levels(as.factor(city))[2] & type == levels(as.factor(type))[2])])
round(s11, digits = 1)
round(s12, digits = 1)
round(s21, digits = 1)
round(s22, digits = 1) #molto bene per quanto riguarda le varianze
bartlett.test(value, as.factor(city):as.factor(type))
#b)
summary(kimono.am)
#notiamo ce 0.335 è un p-value molto alto = > eseguo un test statistico per capire
# se può essere 0
library(mvtnorm)
library(MASS)
library(car)
P <- c(
shapiro.test(value[which(city == levels(as.factor(city))[1] & type == levels(as.factor(type))[1])])$p,
shapiro.test(value[which(city == levels(as.factor(city))[1] & type == levels(as.factor(type))[2])])$p,
shapiro.test(value[which(city == levels(as.factor(city))[2] & type == levels(as.factor(type))[1])])$p,
shapiro.test(value[which(city == levels(as.factor(city))[2] & type == levels(as.factor(type))[2])])$p
)
P
linearHypothesis(kimono.am, c(0,1,0,0), c(0))
#quindi
kimono.am2 <- aov(value ~ type + city:type)
summary(kimono.am2)
fit.am <- aov(value ~ type)
summary(fit.am)
#c)
hm.mean <- mean(value[which(type == 'hand-made')])
rtu.mean <- mean(value[which(type == 'ready-to-use')])
S <- sum((fit.am$residuals)^2)/fit.am$df.residual
n.hm <- length(value[which(type == 'hand-made')])
n.rtu <- length(value[which(type == 'hand-made')])
# entrambe gaussiane per il teo centr del lim
CI <- c(
hm.mean-rtu.mean - sqrt(S*(1/n.hm +1/n.rtu))*qt(1-0.05/2,n.hm+n.rtu-2),
hm.mean-rtu.mean + sqrt(S*(1/n.hm +1/n.rtu))*qt(1-0.05/2,fit.am$df.residual)
)
print(CI)
fit.am <- aov(value ~ type)
summary(fit.am)
# entrambe gaussiane per il teo centr del lim
CI <- c(
hm.mean-rtu.mean - sqrt(S*(1/n.hm +1/n.rtu))*qt(1-0.05/2,n.hm+n.rtu-2),
hm.mean-rtu.mean + sqrt(S*(1/n.hm +1/n.rtu))*qt(1-0.05/2,fit.am$df.residual)
)
print(CI)
